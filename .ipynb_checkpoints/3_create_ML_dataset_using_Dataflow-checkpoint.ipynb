{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f78bda5a-6125-4377-a434-ce97f7cdacdc",
   "metadata": {},
   "source": [
    "# 3_create_ML_dataset_using_Dataflow\n",
    "\n",
    "In this notebook, we'll use Cloud Dataflow to preprocess the entire dataset from BigQuery and create CSV files for the training/evaluation datasets for operationalizing the ML model later.\n",
    "\n",
    "Some benefits of using Dataflow:\n",
    "- Dataflow sets itself apart as a platform for data transformations because it is a serverless, fully managed offering from Google that allows you to execute Data Processing Pipelines at scale.\n",
    "- Dataflow executes our code using the Apache Beam API. Apache Beam supports both batch and streaming processing using the same pipeline code.\n",
    "- Dataflow changes the amount of compute resources, the number of servers that will run your pipeline elastically, all depending on the amount of data that your pipeline needs to process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfd523a-8e59-49a3-a108-4ba2cd88ee93",
   "metadata": {},
   "source": [
    "## Import necessary libraries & Set up environment variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ed4769-fc7b-4403-98ed-9b022d7b6f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user google-cloud-bigquery==1.25.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ba2981-60db-4b9d-871c-9cf8f49521e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user apache-beam[interactive]==2.24.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9e3e9a9-c51f-4bd4-9bc9-ecd975d01a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.24.0\n",
      "2.3.4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas\n",
    "import datetime\n",
    "from google.cloud import bigquery\n",
    "import apache_beam as beam\n",
    "print(beam.__version__)\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e4e9adf-2def-4f1a-b579-929adcf49a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import copy\n",
    "import shutil\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb7e077e-d912-41b7-8b66-497dcd843a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"predict-babyweight-10142021\"\n",
    "BUCKET = PROJECT\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"BUCKET\"] = BUCKET \n",
    "os.environ[\"REGION\"] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf6f869b-fc2c-4474-92ba-d89e6c623afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if ! gsutil ls|grep -q gs://${BUCKET}/; then\n",
    "    gsutil mb -l ${REGION} gs://${BUCKET}\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2a18ad-1551-4683-8826-e7659fc46179",
   "metadata": {},
   "source": [
    "## Call BigQuery and examine in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d21c9a15-c63a-4de0-b651-6adb5d2f9bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "bq = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c68c8428-b500-4cc8-b200-96921463957b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2003 = \"\"\"\n",
    "SELECT\n",
    "    weight_pounds,\n",
    "    CAST(is_male AS STRING) AS is_male,\n",
    "    mother_age,\n",
    "    plurality,\n",
    "    gestation_weeks,\n",
    "    IFNULL(CAST(cigarette_use AS STRING), \"Unknown\") AS cigarette_use,\n",
    "    IFNULL(CAST(alcohol_use AS STRING), \"Unknown\") AS alcohol_use,\n",
    "    year,\n",
    "    month,\n",
    "    COALESCE(wday, day, 0) AS date,\n",
    "    IFNULL(state, \"Unknown\") AS state,\n",
    "    IFNULL(mother_birth_state, \"Unknown\") AS mother_birth_state\n",
    "FROM\n",
    "    publicdata.samples.natality\n",
    "WHERE\n",
    "    year > 2002\n",
    "    AND weight_pounds > 0\n",
    "    AND mother_age > 0\n",
    "    AND plurality > 0\n",
    "    AND gestation_weeks > 0\n",
    "\"\"\"\n",
    "\n",
    "query_2003_with_hash_vals=f\"\"\"\n",
    "SELECT\n",
    "    weight_pounds,\n",
    "    is_male,\n",
    "    mother_age,\n",
    "    plurality,\n",
    "    gestation_weeks,\n",
    "    cigarette_use,\n",
    "    alcohol_use,\n",
    "    ABS(FARM_FINGERPRINT(\n",
    "        CONCAT(\n",
    "            CAST(year AS STRING),\n",
    "            CAST(month AS STRING),\n",
    "            CAST(date AS STRING),\n",
    "            CAST(state AS STRING),\n",
    "            CAST(mother_birth_state AS STRING)\n",
    "        )\n",
    "    )) AS hash_values\n",
    "FROM\n",
    "    ({query_2003})\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7dc3d1-e7a1-4fd1-9edd-f4410c2a2963",
   "metadata": {},
   "source": [
    "Let's find how many records from the `query_2003` call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7defd85-c566-41f9-a79f-1c6ff1e986e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Number of all records found: 25037335\n"
     ]
    }
   ],
   "source": [
    "query_2003_count = f\"SELECT COUNT(*) FROM ({query_2003})\"\n",
    "df_count = bq.query(query_2003_count).to_dataframe()\n",
    "num_records = df_count['f0_'][0]\n",
    "print('*** Number of all records found:',num_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729a3d7b-2e84-497b-8053-d7de1cb28a57",
   "metadata": {},
   "source": [
    "View the `query_2003_with_hash_vals` result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc35a0c9-44fc-4b10-bf82-bc4b8d6d5cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight_pounds</th>\n",
       "      <th>is_male</th>\n",
       "      <th>mother_age</th>\n",
       "      <th>plurality</th>\n",
       "      <th>gestation_weeks</th>\n",
       "      <th>cigarette_use</th>\n",
       "      <th>alcohol_use</th>\n",
       "      <th>hash_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.264232</td>\n",
       "      <td>True</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>8045173873969881371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.563162</td>\n",
       "      <td>True</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>16293285635216904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.209116</td>\n",
       "      <td>False</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>4931362078050829102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.316244</td>\n",
       "      <td>True</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>7429616553140235181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.603743</td>\n",
       "      <td>False</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>1781791737502110095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   weight_pounds  is_male  mother_age  plurality  gestation_weeks  \\\n",
       "0       7.264232     True          22          1               39   \n",
       "1       6.563162     True          38          2               39   \n",
       "2       7.209116    False          18          1               41   \n",
       "3       6.316244     True          25          1               38   \n",
       "4       7.603743    False          23          1               39   \n",
       "\n",
       "  cigarette_use alcohol_use          hash_values  \n",
       "0       Unknown     Unknown  8045173873969881371  \n",
       "1       Unknown     Unknown    16293285635216904  \n",
       "2         false       false  4931362078050829102  \n",
       "3       Unknown     Unknown  7429616553140235181  \n",
       "4       Unknown     Unknown  1781791737502110095  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_limit_100 = bq.query(query_2003_with_hash_vals + \"LIMIT 100\").to_dataframe()\n",
    "df_limit_100.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd90463-a877-4594-bdcd-8f9c9be220f0",
   "metadata": {},
   "source": [
    "## Create ML dataset using Dataflow\n",
    "Let's use Cloud Dataflow to read in the BigQuery data, do some preprocessing, and write it out as CSV files.\n",
    "\n",
    "For the preprocessing, we'll do:\n",
    "\n",
    "- Modify plurality field to be a string where [1, 2, 3, 4, 5] becomes [\"Single(1)\", \"Twins(2)\", \"Triplets(3)\", \"Quadruplets(4)\", \"Quintuplets(5)\"]\n",
    "- Augment our dataset with our three simulated babyweight data by:\n",
    "    - setting all gender information to Unknown and setting plurality of all non-single births to Multiple(2+),\n",
    "    - setting cigarette_use information to Unknown,\n",
    "    - setting alcohol_use information to Unknown.\n",
    "    \n",
    "Fitst, let's define a function to create line(s) of CSV input from columns called by BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8d950c4-997b-42b7-8a84-ffdcc4d54329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_csv(rowdict):\n",
    "    import hashlib\n",
    "    import copy\n",
    "    \n",
    "    CSV_COLUMNS = [\"weight_pounds\",\n",
    "                   \"is_male\",\n",
    "                   \"mother_age\",\n",
    "                   \"plurality\",\n",
    "                   \"gestation_weeks\",\n",
    "                   \"cigarette_use\",\n",
    "                   \"alcohol_use\"]\n",
    "\n",
    "    # Modify plurality field\n",
    "    rowdict_edited = copy.deepcopy(rowdict)\n",
    "    rowdict_edited['plurality'] = ['Single(1)', 'Twins(2)', 'Triplets(3)', 'Quadruplets(4)', 'Quintuplets(5)'][rowdict['plurality'] - 1]\n",
    "    \n",
    "    # Clone data and mask certain columns to simulate lack of ultrasound\n",
    "    no_ultrasound = copy.deepcopy(rowdict_edited)\n",
    "    no_ultrasound['is_male'] = 'Unknown'\n",
    "    no_ultrasound['plurality'] = 'Multiple(2+)' if rowdict['plurality'] > 1 else 'Single(1)'\n",
    "    \n",
    "    # Write out rows for each input row\n",
    "    for result in [rowdict_edited, no_ultrasound]:\n",
    "        data = ','.join([str(result[k]) if k in result else 'None' for k in CSV_COLUMNS])\n",
    "        #key = hashlib.sha224(data.encode('utf-8')).hexdigest()  # hash the columns to form a key\n",
    "        #yield str(f'{data},{key}')\n",
    "        yield str(f'{data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee52817a-c6f7-467c-a8ae-99249274a9b8",
   "metadata": {},
   "source": [
    "Dataflow job will start with a selection BigQuery, converting it to CSV, and writing the output as CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30374e55-1a03-432c-80a0-e5f71a7a9045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(in_test_mode=True):\n",
    "    import shutil, os, subprocess\n",
    "    \n",
    "    HEADER = 'weight_pounds,is_male,mother_age,plurality,gestation_weeks,cigarette_use,alcohol_use'\n",
    "    job_name = 'preprocess-babyweight-features' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S')\n",
    "\n",
    "    if in_test_mode:\n",
    "        print('Launching local job ... hang on')\n",
    "        OUTPUT_DIR = './datasets_preprocessed_Dataflow_test_mode'\n",
    "        shutil.rmtree(OUTPUT_DIR, ignore_errors=True)\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "    else:\n",
    "        print(f'Launching Dataflow job {job_name} ... hang on')\n",
    "        OUTPUT_DIR = f'gs://{BUCKET}/datasets_preprocessed_Dataflow/'\n",
    "        try:\n",
    "            subprocess.check_call(f'gsutil -m rm -r {OUTPUT_DIR}'.split())\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    options = {\n",
    "        'staging_location': os.path.join(OUTPUT_DIR, 'tmp', 'staging'),\n",
    "        'temp_location': os.path.join(OUTPUT_DIR, 'tmp'),\n",
    "        'job_name': job_name,\n",
    "        'project': PROJECT,\n",
    "        'region': REGION,\n",
    "        'teardown_policy': 'TEARDOWN_ALWAYS',\n",
    "        'no_save_main_session': True,\n",
    "    }\n",
    "    \n",
    "    opts = beam.pipeline.PipelineOptions(flags = [], **options)\n",
    "    if in_test_mode:\n",
    "        RUNNER = 'DirectRunner'\n",
    "    else:\n",
    "        RUNNER = 'DataflowRunner'\n",
    "        \n",
    "    p = beam.Pipeline(RUNNER, options = opts)\n",
    "    \n",
    "    query = query_2003_with_hash_vals\n",
    "    if in_test_mode:\n",
    "        query = query + ' LIMIT 100' \n",
    "    \n",
    "    for step in ['train', 'eval', 'test']:\n",
    "        if step == 'train':\n",
    "            selquery = f'SELECT * FROM ({query}) WHERE MOD(hash_values, 100) < 80'\n",
    "        elif step == 'eval':\n",
    "            selquery = f'SELECT * FROM ({query}) WHERE MOD(hash_values, 100) >= 80 AND MOD(hash_values, 100) < 90'\n",
    "        else:\n",
    "            selquery = f'SELECT * FROM ({query}) WHERE MOD(hash_values, 100) > 90'\n",
    "        (p \n",
    "         | f'{step}_read' >> beam.io.Read(beam.io.BigQuerySource(query = selquery, use_standard_sql = True))\n",
    "         | f'{step}_csv' >> beam.FlatMap(to_csv)\n",
    "         | f'{step}_out' >> beam.io.Write(beam.io.WriteToText(os.path.join(OUTPUT_DIR, f'{step}.csv'),header=HEADER))\n",
    "        )\n",
    "    job = p.run()\n",
    "    \n",
    "    if in_test_mode:\n",
    "        job.wait_until_finish()\n",
    "        print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98e55ce-cacf-41e5-a745-8434ea130c8c",
   "metadata": {},
   "source": [
    "First, let's test the `preprocess` function locally to see if it works correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "53eea5fb-cb9c-4d09-8611-e26f6549c1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching local job ... hang on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.io.gcp.bigquery_tools:Dataset predict-babyweight-10142021:temp_dataset_048c92118c49496fa3a5a642693273db does not exist so we will create it as temporary with location=US\n",
      "WARNING:apache_beam.io.gcp.bigquery_tools:Dataset predict-babyweight-10142021:temp_dataset_1f82b2cf5be940bd9a79aae0c261bfbb does not exist so we will create it as temporary with location=US\n",
      "WARNING:apache_beam.io.gcp.bigquery_tools:Dataset predict-babyweight-10142021:temp_dataset_5705186c53e84038a94db1ff756115a7 does not exist so we will create it as temporary with location=US\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "preprocess(in_test_mode = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed9752e6-c733-4653-97e4-22a440058026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_pounds,is_male,mother_age,plurality,gestation_weeks,cigarette_use,alcohol_use\n",
      "6.8673994613,False,27,Single(1),35,Unknown,Unknown\n",
      "6.8673994613,Unknown,27,Single(1),35,Unknown,Unknown\n",
      "5.68572173698,False,27,Single(1),36,false,false\n",
      "5.68572173698,Unknown,27,Single(1),36,false,false\n"
     ]
    }
   ],
   "source": [
    "!head -5 datasets_preprocessed_Dataflow_test_mode/train*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dd1de2-ecb7-4330-9a78-01dedb129ef9",
   "metadata": {},
   "source": [
    "Once everything has run correctly, let's execute the job in Cloud Dataflow. We can monitor the running job at the Dataflow section in the GCP web console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f748059b-5210-49ab-a607-7cff7734ae19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching Dataflow job preprocess-babyweight-features-211023-045809 ... hang on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'teardown_policy': 'TEARDOWN_ALWAYS', 'no_save_main_session': True}\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'teardown_policy': 'TEARDOWN_ALWAYS', 'no_save_main_session': True}\n"
     ]
    }
   ],
   "source": [
    "preprocess(in_test_mode = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d116915-3452-4c6c-b960-9b7fc7c086e1",
   "metadata": {},
   "source": [
    "After ~13 minutes, the job has been done. At this point, we now have the training and evaluation datasets created at scale. The process is also fully automated. We can simply re-run the pipeline periodically to create a new training dataset on fresher data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "49c9a56a-2376-468c-9283-7d55b7f2f671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://predict-babyweight-10142021/datasets_preprocessed_Dataflow/eval.csv-00000-of-00006\n",
      "gs://predict-babyweight-10142021/datasets_preprocessed_Dataflow/eval.csv-00001-of-00006\n",
      "gs://predict-babyweight-10142021/datasets_preprocessed_Dataflow/eval.csv-00002-of-00006\n",
      "gs://predict-babyweight-10142021/datasets_preprocessed_Dataflow/eval.csv-00003-of-00006\n",
      "gs://predict-babyweight-10142021/datasets_preprocessed_Dataflow/eval.csv-00004-of-00006\n",
      "gs://predict-babyweight-10142021/datasets_preprocessed_Dataflow/eval.csv-00005-of-00006\n",
      "gs://predict-babyweight-10142021/datasets_preprocessed_Dataflow/test.csv-00000-of-00003\n",
      "gs://predict-babyweight-10142021/datasets_preprocessed_Dataflow/test.csv-00001-of-00003\n",
      "gs://predict-babyweight-10142021/datasets_preprocessed_Dataflow/test.csv-00002-of-00003\n",
      "gs://predict-babyweight-10142021/datasets_preprocessed_Dataflow/train.csv-00000-of-00020\n",
      "gs://predict-babyweight-10142021/datasets_preprocessed_Dataflow/train.csv-00001-of-00020\n",
      "gs://predict-babyweight-10142021/datasets_preprocessed_Dataflow/train.csv-00002-of-00020\n",
      "gs://predict-babyweight-10142021/datasets_preprocessed_Dataflow/train.csv-00003-of-00020\n",
      "gs://predict-babyweight-10142021/datasets_preprocessed_Dataflow/train.csv-00004-of-00020\n",
      "gs://predict-babyweight-10142021/datasets_preprocessed_Dataflow/train.csv-00005-of-00020\n",
      "gs://predict-babyweight-10142021/datasets_preprocessed_Dataflow/train.csv-00006-of-00020\n",
      "gs://predict-babyweight-10142021/datasets_preprocessed_Dataflow/train.csv-00007-of-00020\n",
      "gs://predict-babyweight-10142021/datasets_preprocessed_Dataflow/train.csv-00008-of-00020\n",
      "gs://predict-babyweight-10142021/datasets_preprocessed_Dataflow/train.csv-00009-of-00020\n",
      "gs://predict-babyweight-10142021/datasets_preprocessed_Dataflow/train.csv-00010-of-00020\n",
      "gs://predict-babyweight-10142021/datasets_preprocessed_Dataflow/train.csv-00011-of-00020\n",
      "gs://predict-babyweight-10142021/datasets_preprocessed_Dataflow/train.csv-00012-of-00020\n",
      "gs://predict-babyweight-10142021/datasets_preprocessed_Dataflow/train.csv-00013-of-00020\n",
      "gs://predict-babyweight-10142021/datasets_preprocessed_Dataflow/train.csv-00014-of-00020\n",
      "gs://predict-babyweight-10142021/datasets_preprocessed_Dataflow/train.csv-00015-of-00020\n",
      "gs://predict-babyweight-10142021/datasets_preprocessed_Dataflow/train.csv-00016-of-00020\n",
      "gs://predict-babyweight-10142021/datasets_preprocessed_Dataflow/train.csv-00017-of-00020\n",
      "gs://predict-babyweight-10142021/datasets_preprocessed_Dataflow/train.csv-00018-of-00020\n",
      "gs://predict-babyweight-10142021/datasets_preprocessed_Dataflow/train.csv-00019-of-00020\n",
      "gs://predict-babyweight-10142021/datasets_preprocessed_Dataflow/tmp/\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil ls gs://${BUCKET}/datasets_preprocessed_Dataflow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "376f78a5-74d8-4e09-9928-3856c3937723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_pounds,is_male,mother_age,plurality,gestation_weeks,cigarette_use,alcohol_use\n",
      "6.87621795178,False,18,Single(1),42,false,false\n",
      "6.87621795178,Unknown,18,Single(1),42,false,false\n",
      "6.81448851842,False,18,Single(1),42,Unknown,Unknown\n",
      "6.81448851842,Unknown,18,Single(1),42,Unknown,Unknown\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil cat gs://predict-babyweight-10142021/datasets_preprocessed_Dataflow/train.csv-00000-of-00020 |head -5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db97467-3bb2-4b8c-b37b-d4afa423ee4e",
   "metadata": {},
   "source": [
    "## Summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6abd1ad-bf9e-448b-8326-fe0c92746121",
   "metadata": {},
   "source": [
    "- Using Dataflow, we performed multiple preproseeing, modifying, and simulating data for the entire dataset and then produced CSV files for the training/evaluation datasets. \n",
    "- These files are storaged in the Cloud bucket and ready for the training a ML model at scale.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m81",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m81"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
